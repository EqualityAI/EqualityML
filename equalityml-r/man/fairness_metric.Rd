% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fairness_evaluation.R
\name{fairness_metric}
\alias{fairness_metric}
\title{Fairness metric}
\usage{
fairness_metric(
  ml_model,
  input_data,
  target_variable,
  protected_variable,
  privileged_class,
  features = NULL
)
}
\arguments{
\item{ml_model}{object, Trained machine learning model}

\item{input_data}{\code{data.frame}, Data for calculating fairness metric}

\item{target_variable}{character, Target variable}

\item{protected_variable}{character, Data column name which contains sensitive information such as gender, race etc...}

\item{privileged_class}{character,  Privileged class from protected variable : "privileged"}

\item{features}{character, Data columns}
}
\value{
Fairness metric score (list)
}
\description{
Evaluate fairness metrics of a binary classification Machine Learning application.
}
\examples{

set.seed(1)
# custom data frame with sex, age and target column names
custom_data <- data.frame(
  sex = c(rep("M", 140), rep("F", 60)),
  age = c(rep(1:20,10)),
  target = c(
  c(rep(c(1, 1, 1, 1, 1, 1, 1, 0, 0, 0),14)),
  c(rep(c(0, 1, 0, 1, 0, 0, 1, 0, 0, 1),6))
  )
)

ml_model <- glm(target ~ sex + age, data = custom_data, family = 'binomial')

fairness_score <- fairness_metric(
  ml_model = ml_model,
  input_data = custom_data,
  target_variable = "target",
  protected_variable = "sex",
  privileged_class = "M"
)
}

% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fairness_evaluation.R
\name{fairness_metric}
\alias{fairness_metric}
\title{Fairness metric}
\usage{
fairness_metric(
  ml_model,
  input_data,
  target_variable,
  protected_variable,
  privileged_class,
  ignore_protected = TRUE
)
}
\arguments{
\item{ml_model}{object, Trained machine learning model}

\item{input_data}{\code{data.frame}, Data for calculating fairness metric}

\item{target_variable}{character, Target variable}

\item{protected_variable}{character, Data column name which contains sensitive information such as gender, race etc...}

\item{privileged_class}{character,  Privileged class from protected variable : "privileged"}

\item{ignore_protected}{bool, if TRUE, ignore protected variable for model explanation}
}
\value{
Fairness metric score (list)
}
\description{
Evaluate fairness metrics of a binary classification Machine Learning application.
}
\examples{

set.seed(1)
# custom data frame with x1, x2 and y column names
custom_data <- data.frame(
  x1 = as.factor(c(rep(1, 500), rep(2, 500))),
  x2 = c(rnorm(500, 400, 40), rnorm(500, 600, 100))
  y = sample(c(0,1), replace=TRUE, size=1000)
)

ADD ml_model

mitigation_result <- bias_mitigation(
  ml_model = ml_model,
  input_data = custom_data,
  target_variable = custom_data$y,
  protected_variable = custom_data$x1,
  privileged_class = 1
)
}
